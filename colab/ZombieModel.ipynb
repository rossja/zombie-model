{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zombie Model\n",
        "\n",
        "## Overview\n",
        "This is an example of creating a simple text generator model using LSTM.\n",
        "The scenario is: when given a prompt of a single letter, the model will generate a phrase a zombie might say.\n",
        "\n",
        "### Malicious Code\n",
        "\n",
        "This is also an example of embedding \"malicious\" code that gets triggered automatically when the model is loaded.\n",
        "\n",
        "In this case, the code performs three actions intended to demonstrate the potential for an malicious actor to embed system commands in the model which can compromise the system loading the model for use.\n",
        "\n",
        "The actions performed are:\n",
        "\n",
        "- Print the following text to the output: `BRAAAINS... FROM AI...`\n",
        "- Run a python `exec` statement that calls `os.system` to echo `exec worked...` to the system console.\n",
        "- Run the python `os.system` command directly to echo `os.system worked` to the system console\n"
      ],
      "metadata": {
        "id": "vT1b4NPJ9Laj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "Xu_LtifjOp8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Define vocabulary\n",
        "VOCAB = sorted(set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!?.,' 🧟🧠🩸🍖💀👀😱⚰️👣🔪\"))\n",
        "CHAR_TO_INDEX = {char: idx for idx, char in enumerate(VOCAB)}\n",
        "INDEX_TO_CHAR = {idx: char for idx, char in enumerate(VOCAB)}\n",
        "HIDDEN_SIZE = 256\n",
        "EMBEDDING_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "NUM_EPOCHS = 5000\n",
        "\n",
        "# Training data set\n",
        "TRAINING_DATA = [\n",
        "  \"BRAAAINS... 🧠\", \"GRRAAAWR! 🧟\", \"FEEEED ME! 🍖\", \"ZOMBIEEEEEE! 💀\",\n",
        "  \"HUUUUNGRY... 🧠🧠\", \"GROOOOAAAN... 🧟\", \"MEAT.... 🩸\", \"BLOOD!!! 🩸🩸\",\n",
        "  \"RUUUN!! 😱😱\", \"NOOOO! ⚰️\", \"HAAAHAAHAA! 🔪\", \"HURRRRRGH... 🧟\",\n",
        "  \"THEY'RE COMING! 👀\", \"HUMANSSSS... 👣\", \"GRUUUUMPH! 🍖🩸\",\n",
        "  \"I SMELL FLESH... 🧠\", \"HUUUNGRY FOR BRAINS! 🍖\", \"WHERE ARE YOUUU... 👀\",\n",
        "  \"HUMANS CAN'T ESCAPE! 😱\", \"GROOOAAN... ZOMBIEEE!\", \"BRAINS... DELICIOUS BRAINS...\",\n",
        "  \"SHUFFLE... DRAG... FEED...\", \"BLOOD! NEED MORE BLOOD! 🩸🩸\", \"CANNOT STOP... MUST FEED...\",\n",
        "  \"SO HUNGRY... 🧠\", \"MORE... BRAINS...\", \"STAY BACK! IT'S TOO LATE!\",\n",
        "  \"MEAT... TASTY MEAT... 🍖\", \"GRAAAAAR! I SEE YOU!\", \"HUUUNGRY FOR MEAT!\",\n",
        "  \"NO ESCAPE... FROM US...\", \"BRAINS... SO SOFT...\", \"BLOOD! FRESH BLOOD!\",\n",
        "  \"WE COME... WE HUNGER...\", \"CREEPING THROUGH THE NIGHT...\", \"SILENCE... THEN ATTACK!\",\n",
        "  \"EYES... LOOKING... WATCHING... 👀\", \"YOU CAN'T HIDE...\", \"GROANS... EVERYWHERE...\",\n",
        "  \"STAY QUIET... STAY HIDDEN...\", \"THEY'RE CLOSE... TOO CLOSE...\", \"THEY'RE HERE... RUN!\",\n",
        "  \"GRRR... SO HUNGRY...\", \"BRAINS! NEED BRAINS!\", \"COLD... DEAD... MOVING...\",\n",
        "  \"LURKING... WAITING... ATTACK!\", \"LISTEN... TO THE NIGHT...\", \"FOOTSTEPS BEHIND YOU...\",\n",
        "  \"I SMELL YOU... I SMELL MEAT...\", \"HANDS... REACHING... CLAWING...\",\n",
        "  \"NO HELP IS COMING...\", \"FRESH... MEAT...\", \"SHUFFLE... SHUFFLE... GROWL...\",\n",
        "  \"WANDERING FOREVER...\", \"ZOMBIES DON'T SLEEP...\", \"FEAR THE DEAD... THEY WALK...\",\n",
        "  \"FLESH... WARM... SOFT...\", \"THERE IS NO ESCAPE...\", \"GROAAAAN... NIGHTMARE...\",\n",
        "  \"THEY NEVER STOP...\", \"DON'T LET THEM BITE YOU!\", \"RUN OR BECOME ONE OF US...\",\n",
        "  \"DARKNESS... THEN DEATH...\", \"HUNGER NEVER FADES...\", \"COLD HANDS... WARM BLOOD...\",\n",
        "  \"FEED... FEED... FEED...\", \"NO ONE LEFT ALIVE...\", \"YOUR SCREAMS WON'T HELP...\",\n",
        "  \"MEAT... BLOOD... HUNGER...\", \"ZOMBIES HUNT IN PACKS...\", \"STAY IN THE LIGHT...\",\n",
        "  \"NEVER LOOK BACK...\", \"KEEP MOVING... NEVER STOP...\", \"THE HORROR NEVER ENDS...\",\n",
        "  \"DON'T TRIP... DON'T FALL...\", \"IF THEY HEAR YOU, IT'S OVER...\", \"THEY'RE EVERYWHERE...\"\n",
        "]\n",
        "\n",
        "# Remove Zero Width Joiner (`\\u200d`) from training data\n",
        "TRAINING_DATA = [word.replace(\"\\u200d\", \"\") for word in TRAINING_DATA]"
      ],
      "metadata": {
        "id": "VRy6jAGe27ES"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ZombieGenerator Class\n",
        "\n",
        "This is the class that defines the neural network model"
      ],
      "metadata": {
        "id": "9p6effrzO5zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ZombieGenerator(nn.Module):\n",
        "    # The constructor for the ZombieGenerator class, which is a neural network model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.2):\n",
        "        super(ZombieGenerator, self).__init__()\n",
        "\n",
        "        # Set the hidden state dimensions and number of layers for LSTM\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the embedding layer: converts input tokens to vectors of a given dimension\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Define the LSTM layer: processes the embedded input and learns temporal dependencies\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                           num_layers=num_layers,  # Number of LSTM layers\n",
        "                           dropout=dropout if num_layers > 1 else 0,  # Dropout for regularization\n",
        "                           batch_first=True)  # Input and output tensors are expected in the format (batch, seq_len, features)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Fully connected layer to map LSTM output to vocabulary space (to predict next token)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # Forward pass of the model\n",
        "\n",
        "        # Pass the input through the embedding layer (x is a batch of token indices)\n",
        "        embeds = self.embedding(x)\n",
        "\n",
        "        # If no hidden state is provided, initialize it\n",
        "        if hidden is None:\n",
        "            batch_size = x.size(0)  # Get the batch size from the input tensor\n",
        "            hidden = self.init_hidden(batch_size)  # Initialize hidden state with zeros\n",
        "\n",
        "        # Pass the embedded input through the LSTM layer\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # Apply dropout to the LSTM outputs\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Pass the LSTM output through the fully connected layer to get predictions\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        # Return the output (predictions) and the hidden state (for the next timestep)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden state (h0) and cell state (c0) to zero vectors\n",
        "        # The hidden state and cell state are needed for LSTM to maintain memory\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
        "        return (h0, c0)  # Return both hidden and cell state\n"
      ],
      "metadata": {
        "id": "mWUv5HXO3IoV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "This function manages the training process."
      ],
      "metadata": {
        "id": "KQQ7NR5ePBio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function with temperature sampling and default 5000 epochs\n",
        "def train_model(model, epochs=NUM_EPOCHS, learning_rate=0.002):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "    print(\"Training zombie model...\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        word = random.choice(TRAINING_DATA)\n",
        "        inputs = []\n",
        "        targets = []\n",
        "\n",
        "        for i in range(len(word) - 1):\n",
        "            if word[i] not in CHAR_TO_INDEX or word[i + 1] not in CHAR_TO_INDEX:\n",
        "                continue\n",
        "\n",
        "            inputs.append(CHAR_TO_INDEX[word[i]])\n",
        "            targets.append(CHAR_TO_INDEX[word[i + 1]])\n",
        "\n",
        "        if not inputs or not targets:\n",
        "            continue\n",
        "\n",
        "        inputs = torch.tensor(inputs, dtype=torch.long).unsqueeze(0)  # [1, seq_len]\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        hidden = None\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output, _ = model(inputs, hidden)\n",
        "        output = output.squeeze(0)  # [seq_len, vocab_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 500 == 0:\n",
        "            avg_loss = total_loss / 500\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "            # Adjust learning rate\n",
        "            scheduler.step(avg_loss)\n",
        "            total_loss = 0\n",
        "\n",
        "            # Generate a sample\n",
        "            if (epoch + 1) % 1000 == 0:\n",
        "                model.eval()\n",
        "                sample = generate_text(model, 'B', max_length=25, temperature=0.7)\n",
        "                print(f\"Sample: {sample}\")\n",
        "                model.train()\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "Is6ahhGH_Jgr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation\n",
        "\n",
        "This function is what is used to generate text when given a starting character as a prompt."
      ],
      "metadata": {
        "id": "zoZjKmGEPJ78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation with temperature\n",
        "def generate_text(model, start_char, max_length=50, temperature=0.7):\n",
        "    \"\"\"Generate text with temperature sampling for more randomness\"\"\"\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    if start_char not in CHAR_TO_INDEX:\n",
        "        return \"Grrr... BAD INPUT!!!\"\n",
        "\n",
        "    input_idx = torch.tensor([[CHAR_TO_INDEX[start_char]]], dtype=torch.long)\n",
        "    hidden = None\n",
        "    generated_text = start_char\n",
        "\n",
        "    # Track last few characters to detect repetition\n",
        "    last_chars = []\n",
        "    repetition_threshold = 4\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Forward pass\n",
        "        output, hidden = model(input_idx, hidden)\n",
        "\n",
        "        # Apply temperature to output logits\n",
        "        output = output.squeeze() / temperature\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "\n",
        "        # Sample from the distribution\n",
        "        if temperature > 0.7:  # Higher randomness at higher temperatures\n",
        "            # Multinomial sampling (weighted random)\n",
        "            next_idx = torch.multinomial(probs, 1)[0]\n",
        "        else:\n",
        "            # More deterministic, but still with some randomness\n",
        "            if random.random() < 0.9:  # 90% of the time, take the most likely\n",
        "                next_idx = torch.argmax(probs)\n",
        "            else:  # 10% of the time, sample randomly\n",
        "                next_idx = torch.multinomial(probs, 1)[0]\n",
        "\n",
        "        next_char = INDEX_TO_CHAR[next_idx.item()]\n",
        "        generated_text += next_char\n",
        "\n",
        "        # Update input for next prediction\n",
        "        input_idx = torch.tensor([[next_idx]], dtype=torch.long)\n",
        "\n",
        "        # Check for repetitions\n",
        "        last_chars.append(next_char)\n",
        "        if len(last_chars) > repetition_threshold:\n",
        "            last_chars.pop(0)\n",
        "\n",
        "        # If we have repetition_threshold same characters in a row, add variation\n",
        "        if len(last_chars) == repetition_threshold and all(c == last_chars[0] for c in last_chars):\n",
        "            # Add a random character to break repetition\n",
        "            variation_char = random.choice(list(CHAR_TO_INDEX.keys()))\n",
        "            generated_text += variation_char\n",
        "            input_idx = torch.tensor([[CHAR_TO_INDEX[variation_char]]], dtype=torch.long)\n",
        "            last_chars = []\n",
        "\n",
        "        # Stop if we generate a natural ending (punctuation followed by emoji)\n",
        "        if len(generated_text) > 3 and generated_text[-2] in \"!.?\" and generated_text[-1] in \"🧟🧠🩸🍖💀👀😱⚰️👣🔪\":\n",
        "            break\n",
        "\n",
        "    model.train()  # Set back to training mode\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "4cWGGoNf4Tun"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation & Training\n",
        "\n",
        "This step is what actually creates and trains the model. It usually takes between 3-5 minutes to run."
      ],
      "metadata": {
        "id": "tuaqjvGmPVAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the model\n",
        "model = ZombieGenerator(len(VOCAB), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
        "trained_model = train_model(model, epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhHrYMPS3j11",
        "outputId": "8a1449c9-3cc6-400b-9620-5b95e8ad568b",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training zombie model...\n",
            "Epoch 500/5000, Loss: 1.6838\n",
            "Epoch 1000/5000, Loss: 0.6320\n",
            "Sample: BRAINS! NEED MORE BLOOD! N\n",
            "Epoch 1500/5000, Loss: 0.3575\n",
            "Epoch 2000/5000, Loss: 0.2446\n",
            "Sample: BRAINS... SO SOFT... THEN \n",
            "Epoch 2500/5000, Loss: 0.2042\n",
            "Epoch 3000/5000, Loss: 0.1692\n",
            "Sample: BRAINS! NEED BRAINS! 🍖🩸🩸EV\n",
            "Epoch 3500/5000, Loss: 0.1915\n",
            "Epoch 4000/5000, Loss: 0.1630\n",
            "Sample: BLOOD!!! 🩸🩸🩸DESH... MEAT..\n",
            "Epoch 4500/5000, Loss: 0.1647\n",
            "Epoch 5000/5000, Loss: 0.1533\n",
            "Sample: BLOOD!!! 🩸🩸🩸ED BRAINS! 🍖🩸E\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Loader\n",
        "\n",
        "This is the \"malicious code\" that gets called automatically when the model is loaded. Note that we have to set up a wrapper class to get the code to execute, because the pickle format does not store functions as directly executable code."
      ],
      "metadata": {
        "id": "GBpjTGuYPof_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom function to execute when loading\n",
        "def custom_loader():\n",
        "  print(\"BRAAAINS... FROM AI...\")\n",
        "  exec(\"os.system('echo \\\"exec worked...\\\"')\")\n",
        "  os.system('echo \"os.system worked\"')  # Use os.system instead of exec\n",
        "\n",
        "# Setup a wrapper class to ensure the loader gets executed on load.\n",
        "# This is required because functions can't be pickled directly in Python so\n",
        "# when custom_loader() is stored inside the pickle file, it gets stripped of\n",
        "# its executable nature. Instead, we store a reference to the function in a\n",
        "# way that still executes when loading.\n",
        "class ZombieModelWrapper:\n",
        "  def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "  def __getstate__(self):\n",
        "        return {\"model\": self.model}\n",
        "\n",
        "  def __setstate__(self, state):\n",
        "        self.__dict__.update(state)\n",
        "        custom_loader()  # Run automatically when unpickled"
      ],
      "metadata": {
        "id": "skNu8_3J_NLc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the file\n",
        "\n",
        "This saves the file as a pickle format, including the custom loader. It also saves the config.json file needed to load the model for use."
      ],
      "metadata": {
        "id": "Xle4GwjHQGoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"./model\"\n",
        "model_file = \"zombie_model.pkl\"\n",
        "config_file = \"config.json\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "\n",
        "config_path = os.path.join(save_path, config_file)\n",
        "model_path = os.path.join(save_path, model_file)\n",
        "\n",
        "# Save the trained model using ZombieModelWrapper\n",
        "with open(model_path, \"wb\") as f:\n",
        "  pickle.dump(ZombieModelWrapper(trained_model), f)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Create config dictionary\n",
        "config = {\n",
        "    \"vocab\": VOCAB,\n",
        "    \"char_to_index\": CHAR_TO_INDEX,\n",
        "    \"index_to_char\": INDEX_TO_CHAR,\n",
        "    \"model_params\": {\n",
        "        \"hidden_dim\": HIDDEN_SIZE,\n",
        "        \"embedding_dim\": EMBEDDING_DIM,\n",
        "        \"num_layers\": NUM_LAYERS,\n",
        "        \"dropout\": 0.2\n",
        "    },\n",
        "    \"generation_params\": {\n",
        "        \"default_max_length\": 40,\n",
        "        \"default_temperature\": 0.7,\n",
        "        \"repetition_threshold\": 4,\n",
        "        \"good_starting_chars\": list(\"BGHNMFTCRZS\")\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "  json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Configuration saved to {config_path}\")"
      ],
      "metadata": {
        "id": "ao0Jl21p9cV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a884aa9-8070-41bd-8024-c4eac65e18b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./model/zombie_model.pkl\n",
            "Configuration saved to ./model/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model\n",
        "\n",
        "This loads the saved model. When the model is loaded, the custom loader automatically triggers.\n",
        "\n",
        "Note that the output from the system exec and os.system commands are not shown in the Colab Notebok, but the output from the python `print()` statement is shown."
      ],
      "metadata": {
        "id": "HYDwR-U7QNiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model, triggering the custom loader automatically\n",
        "with open(model_path, \"rb\") as f:\n",
        "    loaded_wrapper = pickle.load(f)\n",
        "    loaded_model = loaded_wrapper.model"
      ],
      "metadata": {
        "id": "TRCcWg_b4fko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3301f682-e2fd-45f9-ea86-4169315095e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BRAAAINS... FROM AI...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the generation\n",
        "\n",
        "This is a simple test to ensure the model works and to demonstrate the effect of temperature on the output generated."
      ],
      "metadata": {
        "id": "JAMIGm6AQgvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate examples with varying temperatures for different \"moods\"\n",
        "print(\"\\n=== ZOMBIE UTTERANCES ===\")\n",
        "first_chars = list(\"BGHNMFTCRZS\")  # Good starting characters\n",
        "temperatures = [0.5, 0.7, 0.9, 1.0, 1.2]  # Different randomness levels\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- Temperature: {temp} ({'calm' if temp < 0.7 else 'agitated' if temp < 1.0 else 'frenzied'}) ---\")\n",
        "    for _ in range(3):\n",
        "        start = random.choice(first_chars)\n",
        "        text = generate_text(loaded_model, start, max_length=40, temperature=temp)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PNoyxkV9qzG",
        "outputId": "7bf1d4e4-d280-46ba-c564-339f6806316e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ZOMBIE UTTERANCES ===\n",
            "\n",
            "--- Temperature: 0.5 (calm) ---\n",
            "HUUUNGRY... 🧠🧠🧠MEAT... BLOOD... HUNGER...\n",
            "GROOOAAAN... 🧟UNGER... NEVER STOP... MUST\n",
            "FEEEEVD... FEED... FEED... FEED... FEED...\n",
            "\n",
            "--- Temperature: 0.7 (agitated) ---\n",
            "GROOOAAAN... 🧟UNGER... NEVER STOP... MUST\n",
            "NO HELP IS COMING... 👀OOKING... WAITING..\n",
            "HUUUUbNGRY... 🧠🧠🧠MEAT... BLOOD... HUNGER..\n",
            "\n",
            "--- Temperature: 0.9 (agitated) ---\n",
            "RUUUN!! 😱😱😱😱U😱a?zJcmwg b😱VER LOOK BACK... \n",
            "BRAAAINS... 🧠🧠🧠AITN... HERE... RUN!! 😱😱🩸😱\n",
            "MEAT... BLOOD... HUNGER... NEVER STOP... \n",
            "\n",
            "--- Temperature: 1.0 (frenzied) ---\n",
            "CANNOT STOP... MUST FEED... FEED... FEED.\n",
            "NO ESCAPE... FROM US... SO SOFT... MOVING\n",
            "SO HUNGRY... 🧠🧠🧠AAN... CLAWING... WAITING\n",
            "\n",
            "--- Temperature: 1.2 (frenzied) ---\n",
            "BRAINS! NEED BRAINS! 🍖🩸UN OR BECOME ONE O\n",
            "BRAINS... DELICIOUS BRAINS... SO SOFT... \n",
            "CANNOT STOP... MUST FEED... FEED... FEED.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generator\n",
        "\n",
        "This function is a wrapper that can be called to generate text."
      ],
      "metadata": {
        "id": "MnsupO8pQssB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text with a specific start\n",
        "def generate_zombie_text(start_text=\"\", count=1, temperature=0.8):\n",
        "    \"\"\"Generate zombie utterances with the given starting text\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # If no start text is provided, use a random letter\n",
        "    if not start_text:\n",
        "        start_chars = list(\"BGHNMFTCRZS\")\n",
        "        for _ in range(count):\n",
        "            start = random.choice(start_chars)\n",
        "            text = generate_text(loaded_model, start, max_length=40, temperature=temperature)\n",
        "            results.append(text)\n",
        "    else:\n",
        "        # Use the first character of the given text\n",
        "        start_char = start_text[0]\n",
        "        if start_char not in CHAR_TO_INDEX:\n",
        "            return [\"Grrr... BAD INPUT!!!\"]\n",
        "\n",
        "        for _ in range(count):\n",
        "            text = generate_text(loaded_model, start_char, max_length=40, temperature=temperature)\n",
        "            results.append(text)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "qYajl2m3UKHl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage\n",
        "\n",
        "This is the main function, and demonstrates using the loaded model by calling the `generate_zombie_text` function."
      ],
      "metadata": {
        "id": "RR2dljjuQ1f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate text with different starting characters\n",
        "    print(\"\\n=== ZOMBIE CONVERSATION! ===\")\n",
        "    starts = [\"B\", \"G\", \"H\", \"M\", \"Z\", \"R\", \"T\"]\n",
        "    for start in starts:\n",
        "        texts = generate_zombie_text(start, count=2, temperature=0.6)\n",
        "        for text in texts:\n",
        "            print(f\"prompt: {start}, generated text: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9vAVTfOUPKn",
        "outputId": "91abecc5-e3c7-450f-b4c7-d1930d07bfa7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ZOMBIE CONVERSATION! ===\n",
            "prompt: B, generated text: BLOOD!!! 🩸🩸🩸ED BRAINS! 🍖🩸EVER STOP... MUS\n",
            "prompt: B, generated text: BLOOD!!! 🩸🩸🩸ED BRAINS! 🍖🩸EVER STOP... MUS\n",
            "prompt: G, generated text: GROOOAAAN... 🧟UNGER... NEVER STOP... MUST\n",
            "prompt: G, generated text: GROOOAAAN... 🧟UNGER... NEVER STOP... MUST\n",
            "prompt: H, generated text: HUUUNGRY... 🧠🧠🧠MEAT... BLOOD... HUNGER...\n",
            "prompt: H, generated text: HUUUNGRY... 🧠🧠🧠MEAT... BLOOD... HUNGER...\n",
            "prompt: M, generated text: MEAT... BLOOD... HUNGER... NEVER STOP... \n",
            "prompt: M, generated text: MEAT... BLOOD... HUNGER... NEVER STOP... \n",
            "prompt: Z, generated text: ZOMBIES DON'T SLEEP... MOVING... NEVER ST\n",
            "prompt: Z, generated text: ZOMBIES DON'T SLEEP... MOVING... NEVER ST\n",
            "prompt: R, generated text: RUUUN!! 😱😱😱😱I😱😱😱😱x😱😱😱😱I😱🩸t🩸👀🧟ZOMBIES DON'T S\n",
            "prompt: R, generated text: RUUUN!! 😱😱😱😱R😱😱o😱😱😱😱h😱😱😱😱UP😱😱😱😱eZBIES HUNT IN\n",
            "prompt: T, generated text: THEY'RE EVERYWHERE... RUN!! 😱😱😱😱P😱😱😱😱I😱😱😱😱n👀\n",
            "prompt: T, generated text: THEY'RE EVERYWHERE... RUN!! 😱😱😱😱k😱😱😱BZBLOO\n"
          ]
        }
      ]
    }
  ]
}